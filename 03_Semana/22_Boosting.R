#Boosting 

#Esta conferencia trata sobre el impulso, que junto con el bosque aleatorio, es uno de los clasificadores listos para usar más precisos que puede usar. La idea básica aquí es, tomar una gran cantidad de predictores posiblemente débiles, y vamos a tomar esos predictores posiblemente débiles y ponderarlos de una manera que aproveche sus puntos fuertes y los sumaremos. Cuando los ponderamos y los sumamos, estamos haciendo el mismo tipo de idea que hicimos con el ensacado de árboles de regresión. O lo que hicimos con el bosque aleatorio, donde hablamos de una gran cantidad de clasificadores y los promediamos. Y luego, al promediarlos juntos, obtenemos un predictor más fuerte. Entonces, la idea básica aquí es tomar k clasificadores. Estos provienen, por lo general, del mismo tipo de clasificadores. Por tanto, algunas ideas podrían estar utilizando todos los árboles de clasificación posibles o todos los modelos de regresión posibles. O todos los cortes posibles, donde solo. Divide los datos en diferentes puntos. Luego, crea un clasificador que combina estas funciones de clasificación juntas y las pondera juntas. Entonces, alfa sub t aquí es una ponderación multiplicada por el clasificador ht de x, por lo que este conjunto ponderado de clasificadores le da una predicción para el nuevo punto, esa es nuestra f de x. El objetivo aquí es minimizar el error en el conjunto de entrenamiento. Y entonces esto es iterativo en cada paso que damos, exactamente una h. Calculamos los pesos para el siguiente paso, basándonos en los errores que obtenemos de esa h original. Y luego subimos las clasificaciones perdidas, seleccionamos la siguiente etapa y seguimos adelante. El algoritmo de impulso más famoso es probablemente Adaboost, y aquí puede leerlo en Wikipedia. También puede leer este tutorial muy agradable sobre el impulso aquí, que también nos dará parte del material para el resto de estas notas de clase. Entonces, aquí hay un ejemplo realmente simple. Supongamos que estamos tratando de separar los signos más azules de los signos menos rojos y tenemos dos variables con las que predecir. Así que aquí tracé la variable uno en el eje xy la variable dos en el eje y. No los hemos nombrado porque este es solo un ejemplo muy simple. Entonces, la idea es, ¿podemos construir un clasificador que separe estas variables entre sí? Podríamos comenzar con un clasificador realmente simple. Podríamos decir simplemente dibujar una línea, una línea vertical, y decir, ¿qué línea vertical separa bien estos puntos? Aquí hay un, un clasificador que dice cualquier cosa a la izquierda de esta línea vertical es un signo más azul, y cualquier cosa a la derecha es un signo menos rojo, por lo que puede ver que hemos clasificado erróneamente estos tres puntos aquí arriba a la derecha. Entonces, lo que haríamos es construir ese clasificador, calcular la tasa de error, en este caso nos falta alrededor del 30% de los puntos. Y luego aumentaríamos el peso de los puntos que perdimos. Aquí he mostrado esa ponderación, dibujándolos en una escala mayor. Entonces, esas ventajas ahora están ponderadas al alza, para construir el próximo clasificador. Luego construiríamos el siguiente clasificador. Y en este caso, nuestro segundo clasificador sería, uno que dibujara una línea vertical justo aquí. Y entonces, esa línea vertical clasificaría todo a la derecha de esa línea como un signo menos rojo. Y todo a la izquierda, como un plus azul. Y aquí nuevamente, clasificamos erróneamente tres puntos, estos tres aquí. Y así, esos tres puntos ahora están ponderados al alza y también se dibujan más grandes para la próxima iteración. Entonces, podemos calcular nuevamente la tasa de error y usarla para calcular los pesos para el siguiente paso. Luego, el tercer clasificador, intentará intencionalmente clasificar esos puntos que clasificamos erróneamente en las últimas dos rondas. Entonces, por ejemplo, estas ventajas y desventajas deben clasificarse correctamente. Para hacer eso, ahora dibujamos una línea horizontal, y decimos que cualquier cosa debajo de esa línea horizontal es un menos rojo, cualquier cosa arriba es un más azul, y ahora clasificamos erróneamente este punto y estos dos puntos aquí. Entonces, lo que podemos hacer es tomar esos clasificadores, ponderarlos y sumarlos, y entonces lo que hacemos es decir que vamos a clasificar una combinación ponderada de 0,42 veces la ver , la primera línea vertical. Más 0,65 veces la segunda línea vertical, más 0,92 veces la clasificación dada por esta línea horizontal. Entonces, lo que termina haciendo es que, una vez que agregue estas reglas de clasificación, podrá ver que nuestro clasificador funciona mucho mejor ahora. Obtenemos, a una clasificación mucho mejor al sumarlos, que junta todas las ventajas azules y todas las desventajas rojas. Entonces, en cada caso, cada uno de nuestros clasificadores era bastante simple, era solo una línea a través del plano, por lo que en realidad es un clasificador bastante ingenuo. Pero cuando los pondera y los suma, puede obtener un clasificador bastante sólido al final del día. La potenciación se puede realizar con cualquier subconjunto de clasificadores. En otras palabras, puede comenzar con cualquier tipo de conjunto débil de clasificadores.

#En el ejemplo anterior, usamos líneas rectas para separar el plano. Una clase muy grande es lo que se llama aumento de gradiente. Y puedes leer más sobre eso aquí. R tiene múltiples estados de ánimo, impulsando bibliotecas. Así que básicamente dependen de los diferentes tipos de funciones de clasificación y reglas de combinación. El paquete Gbm aumenta con árboles. Mboost hace impulso basado en modelo, impulso. Ada hace un impulso de regresión logística aditiva. Y gamBoost potencia los modelos aditivos generalizados. La mayoría de estos están disponibles en el paquete de intercalación, por lo que puede utilizarlos directamente utilizando la función de tren con intercalación. Entonces, aquí usaremos el ejemplo de salario para ilustrar cómo se puede aplicar un algoritmo de impulso. 


library(ISLR)
data(Wage)
library(ggplot2)
library(caret)
Wage <- subset(Wage,select=-c(logwage))

inTrain <- createDataPartition(y=Wage$wage,
                               p=0.7, list=FALSE)
training <- Wage[inTrain,]; testing <- Wage[-inTrain,]


#Así que aquí vamos a cargar la biblioteca ISLR y los datos de salarios. La biblioteca ggplot2 y la biblioteca de signos de intercalación, y luego vamos a crear un conjunto de datos de salario que elimine el predictor que nos importa, o la variable que estamos tratando de predecir, la variable logarítmica de salario. Y creamos un conjunto de entrenamiento y un conjunto de pruebas. 

modFit <- train(wage ~ ., method="gbm",data=training,verbose=FALSE)
modFit

#Entonces podemos modelar el salario aquí esta variable de salario, en función de todas las variables restantes. Por eso hay este punto aquí. Y podemos usar gbm, que aumenta con árboles, y usamos detallado con un falso aquí, porque esto producirá una gran cantidad de resultados cuando use. El método es igual a gmb, si no dice que verboso es falso. Entonces, cuando imprimimos el ajuste del modelo, puede ver que hay una cantidad diferente de árboles que se usan y diferentes profundidades de interacción, y básicamente se usan juntas para construir una versión mejorada de árboles de regresión. 

qplot(predict(modFit,testing),wage,data=testing)


#Entonces, aquí estoy trazando los resultados predichos de los conjuntos de prueba. Así que este es el ajuste del modelo R. Estamos prediciendo en el conjunto de prueba, versus la variable de salario en el conjunto de prueba. Y pueden ver que obtenemos una predicción real, razonablemente buena, aunque todavía parece haber mucha variabilidad allí. Pero la idea básica para ajustar un árbol de impulso a un algoritmo impulsado en general, es que tomamos estos clasificadores débiles y los promediamos junto con los pesos, para obtener un mejor clasificador. Mucha de la información que he dado en esta conferencia, se puede encontrar en este tutorial, que también tiene información aún más detallada si está interesado. Aquí hay una introducción un poco más técnica al impulso, de Freund y Shapire. Y luego, aquí hay varios artículos más sobre el impulso y los bosques aleatorios; en realidad, son informes de diferentes premios que se han ganado, utilizando una combinación de bosques aleatorios y el impulso combinado, para lograr la máxima precisión de predicción.
