#Prediciendo con Arboles

#Esta conferencia trata sobre predecir con árboles. La idea básica es que si tiene un montón de variables que desea usar para predecir un resultado, puede tomar cada una de esas variables y usarlas para dividir el resultado en diferentes grupos. Y, a medida que divide los resultados en diferentes grupos, puede evaluar la homogeneidad del resultado dentro de cada grupo. Y continúe dividiéndose nuevamente si es necesario, y luego, hasta que obtenga resultados que estén separados en grupos que sean lo suficientemente homogéneos, o que sean lo suficientemente pequeños como para que deba detenerse. Las ventajas de este enfoque son que es fácil de interpretar correctamente y tiende a obtener un mejor rendimiento en entornos no lineales que con los modelos de regresión lineal de los que hablamos en las conferencias anteriores. Las desventajas son que sin excluir o sin algún tipo de validación cruzada, esto puede llevar a un sobreajuste. Y puede ser más difícil estimar la incertidumbre de lo que puede ser para la configuración del modelo de regresión lineal. En general, los resultados pueden ser variables según los valores exactos de los parámetros o las variables que recopiló. Así que aquí hay un ejemplo de cómo se ve un árbol de decisiones después de haber sido construido. Así que este es un árbol de decisiones que apareció en el New York Times durante las elecciones de 2008, cuando el abogado Barack Obama se postuló contra Hillary Clinton por la nominación demócrata para presidente. Y estaban tratando de decidir cuál sería una regla de predicción sobre si un condado votaría por Obama o por Hillary Clinton. Y, entonces, la forma en que sucedió fue que construirían un modelo de predicción que haría preguntas sobre cada una de las diferentes variables que tenían en su conjunto de datos. Entonces, la mejor división resultó ser esta variable. Entonces, si el condado era más del 20% afroamericano, entonces era mucho más probable que el condado votara por Barack Obama. Si era menos del 20% afroamericano, entonces era más probable que el condado votara por Hilary Clinton. Luego, dentro de cada uno de estos dos subgrupos, fueron y buscaron otras variables que pudieran dividir esos subgrupos. Entonces, en esta rama aquí, la siguiente pregunta fue, ¿la tasa de graduación de la escuela secundaria es superior al 78%? Si no fue así, entonces es más probable que el condado vote por Hillary Clinton, y si lo fue, entonces es más probable que vote por Barack Obama. Y el algoritmo continúa de esa manera hasta que se ha dividido en los subgrupos más pequeños, los más pequeños que está dispuesto a considerar. Y, como ves, dentro de cada una de estas hojas del árbol, las predicciones fueron bastante homogéneas. En otras palabras. Obama estaba en 383 de los 450 condados en este caso. Y Hilary Clinton ganó 704 de aproximadamente 790 u 800 condados en este caso. En otras palabras, estas preguntas dividen a los condados en grupos que eran homogéneos dentro de cada hoja, su predicción probablemente se haría realidad de que ese candidato ganaría en las elecciones. Entonces, el algoritmo básico detrás de la construcción de uno de estos árboles es comenzar con todas las variables de un gran grupo. Y luego, encuentra la primera variable que mejor divide los resultados en dos grupos homogéneos diferentes. Luego dividió los datos en dos grupos que llamamos hojas, y así sucesivamente, y la división que acaba de realizar se llama nodo. Dentro de cada división, buscamos nuevamente en todas las variables. Incluyendo la variable en la que acabamos de dividir. Y trate de encontrar dentro de ese grupo si hay otra variable o división que separa el resultado, en grupos aún más homogéneos. Continúas hasta que los grupos sean demasiado pequeños o suficientemente puros. En otras palabras, suficientemente homogéneo. Para detener el algoritmo. Entonces hay diferentes medidas de impureza. Y todos se basan básicamente en esta probabilidad que puedes estimar. Entonces, dentro de un grupo en particular. Entonces, en la hoja le, m, m, entonces tiene n objetos totales que podría considerar, y puede contar el número de veces que una clase en particular aparece en esa hoja. Entonces, este es el número de veces que la clase k aparece en la hoja m. Entonces esa es esta probabilidad. Así que es un sombrero para la hoja m y la clase k. El error de clasificación errónea es 1 menos la probabilidad de que seas igual a la clase más común en esa hoja en particular. Entonces, por ejemplo, si es una hoja donde. Casi todos los condados votarían por Barack Obama, entonces 1 menos el error de clasificación errónea es 1 menos el error de clasificación errónea es 1 menos la probabilidad de que usted vote por Barack Obama. Entonces cero significa pureza perfecta. En otras palabras, no hay error de clasificación errónea y todos los condados irían por Barack Obama. 0.5 significa que no hay pureza porque, no es una, porque en cualquier hoja en particular, si los condados fueran abrumadores, los condados eran abrumadores y probablemente votaran por Hilary Clinton, entonces obtendría una pureza perfecta en la otra dirección.


#Entonces, en realidad, cuando la hoja está perfectamente equilibrada entre los dos resultados diferentes, es cuando no hay homogeneidad dentro de esa hoja en particular. De manera similar, hay algo llamado índice de Gini que no debe confundirse con el coeficiente de Gini en economía. Y es básicamente 1 menos la suma de las probabilidades al cuadrado de que pertenezcas a cualquiera de las diferentes clases. De nuevo, cero significa pureza perfecta. En otras palabras, la clase tiene una clase particular con probabilidad igual a 1 y todas las otras clases tienen probabilidad igual a 0. 0,5 significa que no hay pureza. En otras palabras, todas las clases están perfectamente equilibradas dentro de cada hoja. La desviación y la ganancia de información es otra medida que se puede usar, por lo que se llama desviación si usa log con base e aquí, y log base 2 es la ganancia de información. Y es básicamente menos la suma de la probabilidad. De estar asignado a la clase k y hoja m multiplicado por la base logarítmica 2 o la base e, de esa misma probabilidad. El valor cero significa que hay una pureza perfecta dentro de la hoja, y el valor uno significa que no hay pureza. Si va a este enlace de Wikipedia, tendrá mucha más información sobre cómo se utilizan estas medidas de impureza para separar los valores dentro de cada hoja. 


#Así es como se ven para una pareja, para una s, para un ejemplo específico. Entonces, suponga que tenemos una variable que intenta dividir los puntos en puntos azules y rojos. 

par(mar=c(0,0,0,0))
set.seed(1234)
x <- rep(1:4,each=4)
y <- rep(1:4,4)
plot(x,y,xaxt="n",yaxt="n",cex=3,col=c(rep("blue",15),rep("red",1)),pch=19)


par(mar=c(0,0,0,0)); 
plot(x,y,xaxt="n",yaxt="n",cex=3,col=c(rep("blue",8),rep("red",8)),pch=19)


#Si la variable que se divide de la siguiente manera fuera que 15 de los puntos fueran azules en una hoja en particular y solo uno fuera rojo, esa sería una situación relativamente pura. Entonces, la tasa de clasificación errónea sería uno de cada 16, por lo que ese es solo este punto aquí. Y, por lo tanto, es un valor relativamente bajo, el coeficiente GinI también sería bajo, porque es uno menos 1 dividido por 16 al cuadrado más 15 dividido 16 al cuadrado, que es un número relativamente pequeño y la información obtenida sería similar, en este caso. & Uh, más pequeño hacia 0, porque es 1 sobre 16 veces la base logarítmica 2, 1 sobre 16 más 15 dividido por 16 veces la base logarítmica 2 15 sobre 16. Así que ese es el caso donde hay un resultado relativamente puro en los dos grupos. También podemos ver el caso en el que no lo es. Entonces, suponga que esta variable divide los resultados en una hoja donde la mitad de los valores eran azules y la mitad de los valores eran rojos. Así que esa no sería una muy buena división, porque básicamente es un lanzamiento de moneda si estás de una clase a otra en esa hoja. Entonces, esta clasificación aquí es .5, es muy grande. El índice de Gini también está maximizado, es .5, y la información también es grande, es uno. Entonces, esta sería una división que no se haría, porque no separaría muy bien a los dos grupos en esa hoja en particular. Así que solo les voy a mostrar un ejemplo con los datos del iris.

data(iris) 
library(ggplot2)
names(iris)
table(iris$Species)


#Este es un conjunto de datos muy antiguo, pero le muestra la idea de cómo funciona. Entonces, esto, estoy cargando los datos con el iris de datos de comando. Y luego estoy cargando la biblioteca ggplot2 para hacer gráficos. Entonces, los nombres de este conjunto de datos son las diferentes variables que usaremos para predecir. Aquí está la longitud del sépalo, Sepal.Width, Petal.Length y Petal.Width y lo que estamos tratando de predecir es la especie. 

inTrain <- createDataPartition(y=iris$Species,
                               p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(training)
dim(testing)


#Entonces, hay 50 de estas tres especies diferentes que estamos tratando de predecir con esas variables. Entonces, nuevamente, siempre separo los datos en el entrenamiento y el conjunto de pruebas, y en este caso tengo 45 ejemplos en mi conjunto de entrenamiento que puedo usar para construir un modelo de predicción. Y lo primero que hago es trazar el ancho del pétalo contra el ancho del sépalo. 

qplot(Petal.Width,Sepal.Width,colour=Species,data=training)



#Así que ese es el ancho del pétalo en el eje x, el ancho del sépalo en el eje y, y luego lo voy a colorear por las diferentes especies, y pueden ver que hay tres grupos muy distintos aquí. Entonces, es un problema de clasificación relativamente fácil. Puede ser un poco desafiante para un modelo lineal, pero no necesariamente desafiante para este modelo más avanzado con rastreo de clasificación. Para que pueda ajustar el modelo utilizando la función de entrenamiento en intercalación. De nuevo, estoy entrenando al modelo. El resultado es la especie. Estoy prediciendo con todas las variables restantes, por lo que tengo esta tilde y luego un punto. Y le digo que el método es rpart, que es un paquete para hacer árboles de regresión y clasificación, uno de los paquetes. Y te digo que uses los datos de entrenamiento. Si miro el modelo final, me dice cuáles son todos los nodos y cómo están divididos y en orden, y cuál es la probabilidad de estar en cada clase para cada división. 


library(caret)
modFit <- train(Species ~ .,method="rpart",data=training)
modFit$finalModel

#Entonces, aquí, por ejemplo, hay una división que dice petal.length es menor que 2.45, y si eso sucede, en ese caso, todos los ejemplos que tienen una longitud de pedal menor que 2.45 pertenecen a esta bc setosa. Por lo tanto, puede leer esas divisiones de modelos para decirle qué está haciendo el árbol de clasificación. También puede hacer un gráfico del árbol de clasificación.

plot(modFit$finalModel, uniform=TRUE, 
     main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)


#Entonces, si solo traza el modelo final que produce, lo que hará es producir lo que se llama dandergram, como este. Y es un poco difícil de leer, está cortado aquí, pero puedes ver una longitud de pétalo menor a 2,45. Entonces se le asigna una setosa. Y entonces puede seguir a la izquierda lo que sucede si la longitud del pétalo es menor que 2,45, y a la derecha lo que sucede si la longitud del pétalo no es menor que 2,45. Y luego siga la siguiente división aquí en este nodo hasta el, vea la clasificación total para cualquier ejemplo en particular. Se puede hacer una versión más bonita de esa trama con el paquete rattle. Entonces, usa la función fancyRpartPlot y le pasa el modelo final que se ajustó usando el símbolo de caret. 


library(rattle)
fancyRpartPlot(modFit$finalModel)


#Y vuelve a hacer dentigrama, pero ahora es un poco más fácil de ver, así que aquí vemos. Que si la longitud del pétalo es menor que 2.5, nos movemos aquí a la izquierda, y si es mayor que 2.5, entonces vamos aquí a la derecha. Y luego, dentro de esa división, una vez que ya haya tomado esa decisión sobre esas muestras, y si la longitud del pétalo es menor que 4.8, baje aquí a la izquierda, y si la longitud del pétalo es mayor, vaya aquí al Derecha. Y esto hace que sea un poco más fácil ver lo que está pasando. Puede predecir nuevos valores utilizando la función de predicción, al igual que puede hacerlo con los otros modelos de regresión lineal. 

predict(modFit,newdata=testing)

#Aquí, la predicción será una etiqueta de clase particular, porque el árbol de clasificación se creó para predecir una clase en particular. Y, entonces, cuando paso los datos nuevos, los datos de prueba, al modelo. En realidad, escribirá las diferentes categorías de especies, porque en realidad está prediciendo la clase para cada variable. Entonces, notas y recursos adicionales, los árboles de clasificación son modelos no lineales, por lo que usan inmediatamente interacciones entre variables, esto es importante tenerlo en cuenta, porque siempre es un modelo que se basa en la relación entre múltiples variables y si tiene un gran número de clases para una variable en particular, por ejemplo, con la que está prediciendo. Los modelos pueden sobreajustarse un poco. Las transformaciones de datos pueden ser un poco menos importantes. Si realiza una transformación monótona de una variable continua, en otras palabras, cualquier transformación que no cambie el orden de los valores, pero quizás los haga más grandes o más pequeños. Luego obtendrá las mismas divisiones de datos, con los árboles de clasificación o regresión. Esto puede hacer que las transformaciones sean un poco menos importantes. También se pueden utilizar para problemas de regresión. Entonces, mostré medidas de impureza de clasificación errónea, también puede usar el error cuadrático medio como una medida de impureza. Y también realice un procedimiento de construcción de árbol de clasificación similar para construir modelos de regresión. Hay varias opciones de construcción de árboles en RR, tanto en el paquete de intercalación como en el paquete de fiesta, el paquete rpart. O incluso puede usar este árbol de paquetes que no aparece en el paquete de intercalación, pero también tengo muchas funciones útiles y útiles que se pueden usar al construir estos modelos. Para obtener más información, puede leer estos dos libros, donde hay mucha buena información sobre los árboles de clasificación y regresión. O, en este libro sobre, eso está más específicamente dirigido a este algoritmo de predicción en particular.
