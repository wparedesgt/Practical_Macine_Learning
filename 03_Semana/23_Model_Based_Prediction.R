#Modelos Basados en Predicciones

#Esta conferencia trata sobre la predicción basada en modelos. La idea básica aquí es que vamos a asumir que los datos siguen un modelo probabilístico específico. Luego, usaremos la base therom para identificar clasificadores óptimos basados en ese modelo probabilístico. La ventaja es que este enfoque puede aprovechar alguna estructura que pueda aparecer en los datos. Por ejemplo, la caída de la distribución. Y eso puede conducir a algunas conveniencias computacionales. También puede haber una precisión razonable sobre problemas reales, en particular los problemas reales que parecen seguir la distribución de datos que subyace a todo nuestro modelo holístico. Las desventajas son que hacen estas suposiciones adicionales sobre los datos y no tienen que satisfacerse exactamente para que los algoritmos de predicción funcionen muy bien. Pero si están muy lejos, los algoritmos pueden fallar. Y cuando el modelo es incorrecto, se reduce la precisión. Así que aquí está el enfoque basado en modelos. Nuestro objetivo es construir un modelo paramétrico o un modelo basado en distribuciones de probabilidad. Para la distribución condicional, la probabilidad de que y nuestro resultado sea igual a alguna clase específica k dado un conjunto particular de variables predictoras de modo que nuestras x variables sean iguales al pequeño valor de x. Un enfoque típico es aplicar un teorema de Bayes. En otras palabras, queremos saber algo sobre la probabilidad y es igual a k de que y provenga de la clase k dadas las variables que hemos observado. Y escribimos eso usando el teorema de Bay como la probabilidad X es igual a X dado Y es igual a K multiplicado por la probabilidad Y es igual a K dividido por la ley de probabilidad total aquí abajo. Si no recuerda el teorema de Bay de su clase de inferencia en la especialización en ciencia de datos, puede ir y leerlo en esta página de Wikipedia. Luego asumimos algún modelo paramétrico para la distribución de las características dada la clase, por lo que ese es este componente aquí en sub k de x y asumimos a priori que cada elemento en particular proviene de una clase específica. Eso se denota aquí por este pi sub k. Entonces, básicamente podemos modelar la distribución, la probabilidad de que y sea igual a k dado un conjunto particular de variables predictoras como, esta, fracción aquí donde tenemos un modelo para las x variables y un modelo para la probabilidad previa. Las probabilidades previas, pi k, generalmente se establecen de antemano a partir de los datos. Y luego una elección común para fk de x es una distribución gaussiana. Puede ser una distribución gaussiana multivariante si hay múltiples x variables. Y luego podríamos estimar los parámetros, mu k y sigma al cuadrado k, a partir de los datos. Luego, una vez que tenemos estos parámetros estimados, podemos calcular la probabilidad y pertenece a cualquier clase dada, tan pronto como observemos las variables predictoras. Y clasificamos las variables a la clase de mayor probabilidad en este sentido. Entonces, una variedad de modelos utilizan este enfoque. El más popular de los cuales es probablemente el análisis discriminante lineal, que asume que 'f' 'k' de 'x' es una distribución gaussiana multivariante, por lo que las características tienen una distribución gaussiana multivariante dentro de cada clase y existe la misma matriz de covarianza para cada clase. Esto termina dibujando básicamente líneas a través de los datos, el espacio de covariables. Y por eso se llama análisis discriminante lineal, lo veremos en un minuto. El análisis discriminante cuadrático es muy parecido al análisis discriminante lineal, aunque permite diferentes matrices de covarianza dentro de cada una de las diferentes clases. Y eso termina dibujando curvas cuadráticas a través de los datos, en lugar de líneas. La predicción de la base del modelo básicamente permite versiones más complicadas de la matriz de covarianza cuando construimos estos modelos. Y los clasificadores ingenuos de Bayes, de los que hablaremos un poco más en esta conferencia, básicamente asumen independencia entre las características. Entonces, en otras palabras, asumimos la independencia entre nuestras variables predictoras a través del proceso de construcción del modelo. Esto puede que no sea verdad. Puede que no creamos que las características o los predictores son independientes, pero aún así podría ser un modelo útil para propósitos de predicción. Entonces, ¿por qué se llama análisis discriminante lineal? Bueno, si consideramos la razón de las probabilidades de las dos clases. Entonces, esta es la probabilidad de que estés en la clase K. Dadas nuestras variables predictoras, dividido por la probabilidad aquí en la clase j dadas las variables predictoras. Y tomamos el registro de esa cantidad, así que si tomamos el registro de ella, es una función monótona, lo que significa eso. A medida que aumenta esta ración, también aumentará el logaritmo de esa relación. Entonces, podemos mirar esta cantidad, y podemos ver que se descompone básicamente escribiendo estas cantidades usando el teorema base. Como en la página anterior, obtenemos el logaritmo de la relación de las dos densidades gaussianas. Más logaritmo de la razón de las dos probabilidades previas. Ahora, estos términos en realidad requieren más escritura, así que podemos escribirlos y terminamos con este término aquí.

#Obtenemos el logaritmo de la razón de las probabilidades previas. Más un término aquí que depende de los parámetros de nuestras distribuciones gaussianas o son distribuciones normales para cada clase. Más un término lineal, por lo que esta a, una variable x aquí multiplicada por un coeficiente aquí, que es un término lineal, por lo que termina obteniendo básicamente líneas que se dibujan a través de los datos. Y una variable tendrá una probabilidad más alta de una clase si está en un lado de la línea y una probabilidad más alta de estar en otra clase si está en el otro lado de la línea. Si esto pasó por tu cabeza demasiado rápido, no te preocupes demasiado por eso, pero puedes ir y leerlo con más atención en los elementos de aprendizaje estadístico si deseas saber un poco más sobre esos detalles. Así que esto es lo que tienden a mirar los límites de decisión, como, para este tipo de modelos de predicción. Así que imagina que tenemos tres grupos diferentes de puntos. Entonces estamos tratando de clasificar en clase uno, clase dos o clase tres. Y tenemos dos variables que estamos usando para clasificar y ese es el eje xy el eje y aquí. Lo que terminaría, lo que terminaríamos haciendo es ajustar una distribución gaussiana. Aquí hay una distribución gaussiana. Aquí hay otra distribución gaussiana. Y aquí hay una tercera distribución gaussiana. Entonces uno para cada clase. Y luego básicamente dibujaríamos líneas donde la probabilidad cambia de ser. Superior en esta clase a esa clase. Entonces, eso termina pareciendo líneas como esta. Entonces, si estás en este lado de la línea, serás clasificado como dos. Si estás en este lado de la línea, serás clasificado como un tres. Y si estás aquí abajo, serás clasificado como uno. Básicamente, así es como funciona. Básicamente, ajusta las distribuciones gaussianas a los datos y luego usa esas distribuciones gaussianas para dibujar líneas que asignan los puntos de apoyo a las probabilidades posteriores más altas. En general, la función de discriminación es la que se utiliza aquí. Entonces, la idea básica es que tenemos una función de discriminación que se ve así, donde u, uk es la media de la clase K para todas nuestras características. Sigma inversa es la inversa de la matriz de covarianza para esa clase. En realidad, es lo mismo en la matriz de covarianza para todas las clases aquí. Y luego este es el término, el término lineal en x, los predictores que tenemos y nuestra matriz de covarianza y la media. Entonces, básicamente, lo que hacemos es conectar nuestro nuevo valor de datos en esta función. Y elegimos el valor de k que produce el mayor valor de esta función de discriminación en particular. Y así es como decidimos una clase. Por lo general, puede estimar estos parámetros mediante la estimación de máxima verosimilitud. Si lo recuerda de la inferencia que habría tomado como parte de la especialización en ciencia de datos. Ahora la base hace algo un poco más o más para simplificar el problema. Entonces, nuevamente, suponga que ahora estamos tratando de modelar la probabilidad de que y esté en una clase particular, k. Y tenemos un montón de variables con las que queremos predecir. Podemos usar el teorema de Bayes para decir que la probabilidad de que la clase sea K dadas todas estas variables que hemos observado es la probabilidad previa de que estemos en la clase k multiplicada por la probabilidad de todas estas características dado que estás en la clase k dividida por alguna constante. Entonces, la forma en que esto se puede escribir es que esta probabilidad es proporcional a. La probabilidad anterior multiplicada por la probabilidad de las características, dado que estamos en la clase K. En otras palabras, si eligió el valor más grande de esta cantidad aquí, será lo mismo que elegir la probabilidad más grande aquí. Porque el término en el denominador es solo una constante para todas las diferentes probabilidades. Entonces podemos desglosar eso un poco más. Podemos decir que la probabilidad de las características y la variable de clase k es igual a esta probabilidad previa multiplicada por la probabilidad de x uno dada aquí en la clase k. Multiplica la probabilidad de, todas las demás variables excepto X1, condicionada a Xi e YK. Entonces, esto es básicamente una declaración sobre la probabilidad que puede recordar de su clase de inferencia y la especialización en ciencia de datos. Puede continuar desglosando de la misma manera hasta que obtenga un término para cada función. Pero esas características siempre están condicionadas a todas las demás variables que usted, que le han precedido. Y eso es básicamente porque cada característica puede, o cada uno de los predictores puede depender entre sí. Una suposición que podría hacer para hacer esto un poco más fácil sería simplemente asumir que todas las variables predictoras son independientes entre sí. En cuyo caso abandonan este argumento condicionante. Y termina con la probabilidad anterior multiplicada por la probabilidad de cada característica por sí misma, condicionada a estar en cada clase.

#Ahora bien, esta es una suposición ingenua porque asumimos que todas las características son independientes aunque sabemos que probablemente no lo sean. Y es por eso que este método tiene el título Naive Bayes. Por tanto, sigue funcionando razonablemente bien en una gran cantidad de aplicaciones. Y es particularmente útil cuando tiene una gran cantidad de características que son, binarias o son, variables categóricas. Este get, aparece con mucha frecuencia. Clasificación de texto y clasificación de otro tipo de clasificación de documentos. Entonces, solo para mostrarles brevemente cómo funcionan estos dos, les mostraré los datos de Iris nuevamente y cargaré la biblioteca ggplot2, tengo estas variables que estoy usando para predecir y nuevamente tengo estas, tres especies diferentes que estoy tratando de predecir. Y puedo colocarlos en un conjunto de entrenamiento y aplicarlos a un conjunto de prueba. Al igual que hago con todos nuestros otros algoritmos de predicción. 

data(iris)
library(ggplot2)
names(iris)
table(iris$Species)


inTrain <- createDataPartition(y=iris$Species,
                               p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]

dim(training)

dim(testing)

#Entonces, nuevamente uso la partición de creación de datos. Crea un conjunto de entrenamiento. Y un equipo de prueba. Luego puedo construir un modelo lda en el conjunto de entrenamiento, de la siguiente manera. 

modlda = train(Species ~ .,data=training,method="lda")
modnb = train(Species ~ ., data=training,method="nb")
plda = predict(modlda,testing)
pnb = predict(modnb,testing)




#Básicamente, uso la función trains del paquete caret. Le paso el set de entrenamiento, y le cuento método lda. Y luego puedo hacer lo mismo para la base ingenua, y nb es el método por el que pasamos, una clasificación de base ingenua. Y puedo predecir los valores de lda y de una base ingenua en el conjunto de prueba y hacer una tabla de las predicciones.


table(plda,pnb)

#Y podemos ver que las predicciones coinciden en todos los valores menos uno. Entonces, aunque sabemos que las características o los predictores son dependientes aquí. El uso de la clasificación basada en ingenuo significa reglas de predicción muy similares a la clasificación de análisis de discriminación lineal, por lo que podemos hacer una comparación de los resultados y vemos que solo este valor, el valor que aparece aquí entre las dos clases parece no estar clasificado en de la misma manera por las dos salidas pero en general funcionan de manera muy similar. 

equalPredictions = (plda==pnb)
qplot(Petal.Width,Sepal.Width,colour=equalPredictions,data=testing)

#Puede obtener más información sobre la clasificación basada en nieve y otra clasificación basada en modelos en la introducción al aprendizaje estadístico y los elementos de los libros de aprendizaje estadístico. También puede obtener más información sobre la agrupación en clústeres basada en modelos en este artículo académico al que me he vinculado. Y las páginas de Wikipedia de análisis discriminante lineal y análisis discriminante cuadrático. También son bastante buenos y útiles para Reference.
